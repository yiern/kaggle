# Titanic: Machine Learning from Disaster

[Competition Page](https://www.kaggle.com/competitions/titanic) | [Kaggle Score: 0.76076](https://www.kaggle.com/competitions/titanic/leaderboard)

Binary classification predicting passenger survival on the RMS Titanic using XGBoost with Optuna hyperparameter tuning.

---

## ğŸ¯ Strategy Overview

| Problem | Solution | Why It Works |
|---------|----------|--------------|
| Missing data (Age, Embarked, Fare, Cabin) | Group-based imputation (Age by Pclass+Sex, mode for Embarked, median by Pclass for Fare, HasCabin binary) | Preserves relationships between variables and avoids bias from simple mean/median imputation |
| Non-linear relationships between features and survival | Gradient boosting with XGBoost | Captures complex patterns and interactions that linear models miss |
| High-dimensional hyperparameter space | Optuna with TPESampler (100 trials) | Efficiently explores parameter space using Bayesian optimization |
| Feature sparsity | Feature engineering (Title, FamilySize, IsAlone, FarePerPerson, HasCabin) | Creates meaningful predictors from raw data that improve model performance |

**Core Approach**: XGBoost classifier with automated hyperparameter tuning via Optuna, combined with thoughtful feature engineering and robust missing data handling.

---

## ğŸ—ï¸ Model Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Raw Data (CSV) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Preprocessing Pipeline      â”‚
â”‚  â€¢ Null imputation           â”‚
â”‚  â€¢ Feature engineering       â”‚
â”‚  â€¢ Categorical encoding      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Train/Validation Split      â”‚
â”‚  (80/20, stratified)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  XGBoost Classifier          â”‚
â”‚  â€¢ 10-fold CV                â”‚
â”‚  â€¢ Optuna hyperparam tuning  â”‚
â”‚  â€¢ 100 trials (TPESampler)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Predictions & Evaluation    â”‚
â”‚  â€¢ Confusion matrix          â”‚
â”‚  â€¢ Feature importance        â”‚
â”‚  â€¢ Learning curves           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### XGBoost Hyperparameters

| Parameter | Description | Tuning Range |
|-----------|-------------|--------------|
| `n_estimators` | Number of boosting rounds | 50 - 500 |
| `max_depth` | Maximum tree depth | 3 - 10 |
| `learning_rate` | Step size shrinkage | 0.01 - 0.3 |
| `subsample` | Fraction of samples for each tree | 0.5 - 1.0 |
| `colsample_bytree` | Fraction of features for each tree | 0.5 - 1.0 |
| `min_child_weight` | Minimum sum of instance weight | 1 - 10 |
| `gamma` | Minimum loss reduction | 0 - 5 |
| `reg_alpha` | L1 regularization | 0 - 1 |
| `reg_lambda` | L2 regularization | 0 - 1 |

*All hyperparameters tuned via Optuna with TPESampler*

---

## ğŸ“… Version History & Timeline

| Version | Date | Changes | Score |
|---------|------|---------|-------|
| **v1.0** | Feb 2026 | Initial release with XGBoost + Optuna | **0.76076** |

---

## ğŸ“Š Feature Engineering Pipeline

### 1. Null Handling

```python
# Age: Median by Pclass + Sex groups
df["Age"] = df.groupby(["Pclass", "Sex"])["Age"].transform(
    lambda x: x.fillna(x.median())
)

# Embarked: Mode imputation
df["Embarked"] = df["Embarked"].fillna("S")

# Fare: Median by Pclass
df["Fare"] = df.groupby("Pclass")["Fare"].transform(
    lambda x: x.fillna(x.median())
)

# Cabin: Binary feature (HasCabin)
df["HasCabin"] = df["Cabin"].notna().astype(int)
df = df.drop("Cabin", axis=1)
```

### 2. Feature Creation

```python
# Extract title from Name
df["Title"] = df["Name"].str.extract(r" ([A-Za-z]+)\.", expand=False)
rare_titles = ["Lady", "Countess", "Capt", "Col", "Don", "Dr", 
               "Major", "Rev", "Sir", "Jonkheer", "Dona"]
df["Title"] = df["Title"].replace(rare_titles, "Rare")
df["Title"] = df["Title"].replace({"Mlle": "Miss", "Ms": "Miss", "Mme": "Mrs"})

# Family features
df["FamilySize"] = df["SibSp"] + df["Parch"] + 1
df["IsAlone"] = (df["FamilySize"] == 1).astype(int)
df["FarePerPerson"] = df["Fare"] / df["FamilySize"]
```

### 3. Categorical Encoding

```python
from sklearn.preprocessing import LabelEncoder

encoder = LabelEncoder()
df["Sex"] = encoder.fit_transform(df["Sex"])
df["Title"] = encoder.fit_transform(df["Title"])
df["Embarked"] = encoder.fit_transform(df["Embarked"])
```

**Final Feature Count**: 12 features (7 base + 5 engineered)

---

## ğŸ“ Training Strategy

### Cross-Validation

```python
from sklearn.model_selection import StratifiedKFold, cross_val_score

cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)
scores = cross_val_score(model, X_train, y_train, cv=cv, scoring="accuracy")
```

### Hyperparameter Optimization with Optuna

```python
import optuna
from optuna.samplers import TPESampler

def objective(trial):
    params = {
        "n_estimators": trial.suggest_int("n_estimators", 50, 500),
        "max_depth": trial.suggest_int("max_depth", 3, 10),
        "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3),
        "subsample": trial.suggest_float("subsample", 0.5, 1.0),
        "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
        "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
        "gamma": trial.suggest_float("gamma", 0, 5),
        "reg_alpha": trial.suggest_float("reg_alpha", 0, 1),
        "reg_lambda": trial.suggest_float("reg_lambda", 0, 1),
    }
    
    model = XGBClassifier(**params, random_state=42)
    score = cross_val_score(model, X_train, y_train, cv=10, scoring="accuracy").mean()
    return score

study = optuna.create_study(direction="maximize", sampler=TPESampler(seed=42))
study.optimize(objective, n_trials=100)
```

### Key Training Components

- **Metric**: Accuracy (primary), with confusion matrix analysis
- **Validation**: 10-fold stratified cross-validation
- **Optimization**: Optuna with TPESampler (100 trials)
- **Random State**: 42 (for reproducibility)

---

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ titanic/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ test.csv
â”‚   â””â”€â”€ gender_submission.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ feature_engineering.py
â”‚   â”œâ”€â”€ evaluation/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ metrics.py
â”‚   â””â”€â”€ models/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ xgboost_model.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ xgboost.pkl
â”‚   â””â”€â”€ xgboost_best.pkl
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ confusion_matrix.png
â”‚   â”œâ”€â”€ feature_importance.png
â”‚   â”œâ”€â”€ hyperparameter_tuning.png
â”‚   â””â”€â”€ learning_curve_baseline.png
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_feature_engineering.py
â”œâ”€â”€ train.py
â”œâ”€â”€ predict.py
â”œâ”€â”€ AGENTS.md
â””â”€â”€ README.md
```

---

## âš™ï¸ Configuration

Key configuration parameters in `train.py`:

| Parameter | Value | Description |
|-----------|-------|-------------|
| `RANDOM_STATE` | 42 | Random seed for reproducibility |
| `TEST_SIZE` | 0.2 | Train/validation split ratio |
| `CV_FOLDS` | 10 | Number of cross-validation folds |
| `OPTUNA_TRIALS` | 100 | Number of Optuna optimization trials |
| `OPTUNA_TIMEOUT` | 600 | Optuna optimization timeout (seconds) |

---

## ğŸ”‘ Key Learnings

### What Worked âœ…

- **Optuna for hyperparameter tuning**: Automated 100-trial Bayesian optimization efficiently explored the hyperparameter space, finding configurations that outperformed manual tuning
- **XGBoost's gradient boosting**: Captured non-linear patterns and feature interactions that linear models (e.g., Logistic Regression) missed
- **Feature engineering**: Title extraction (Mr, Mrs, Miss, Rare) proved highly predictive; FamilySize and IsAlone captured social dynamics
- **Group-based imputation**: Filling missing Age values based on Pclass and Sex groups preserved demographic patterns better than global median imputation
- **Stratified cross-validation**: Ensured balanced class distribution across folds, providing reliable performance estimates

### What Didn't Work âŒ

- *N/A - This is the initial version (v1.0)*

---

## ğŸ“ˆ Performance

| Metric | Value |
|--------|-------|
| **Kaggle Submission Score** | **0.76076** |
| Training Samples | 891 |
| Test Samples | 418 |
| Features | 12 (7 base + 5 engineered) |
| Model | XGBoost with Optuna-tuned hyperparameters |
| Validation Strategy | 10-fold stratified cross-validation |

---

## ğŸ› ï¸ Development Commands

```bash
# Install dependencies
pip install -r requirements.txt

# Run training (XGBoost with Optuna hyperparameter tuning)
python train.py

# Generate submission file
python predict.py

# Run all tests
pytest

# Run single test
pytest tests/test_feature_engineering.py

# Run tests with coverage
pytest --cov=. --cov-report=html

# Linting and formatting
ruff check . --fix
ruff format .
```

---

## ğŸ“„ License

This project is licensed under the MIT License.

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

**Built with â¤ï¸ using XGBoost, Optuna, and scikit-learn**

# Store Sales Time Series Forecasting

**Kaggle Competition**: [Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting)

Predict daily sales for 54 Favorita stores across Ecuador using a meta-learning multi-task LSTM architecture.

---

## ğŸ¯ Strategy Overview

### Core Approach: Meta-Learning Multi-Task LSTM

This project implements a **hierarchical meta-learning architecture** that leverages:

1. **Shared Knowledge** - A global LSTM learns temporal patterns common across all 54 stores
2. **Store Adaptation** - Store-specific adapters (54 modules) learn local adjustments
3. **Multi-Task Learning** - Predicts all 33 product families simultaneously
4. **3D Tensor Representation** - `[Batch, Window, Features, Families]` for efficient family-wise feature modeling

### Why This Works

| Problem | Solution |
|---------|----------|
| **Store heterogeneity** | Store-specific adapters capture local patterns |
| **Feature scale mismatch** | Feature encoder (32â†’16 dims) acts as regularization |
| **Family interdependence** | Multi-task output learns family correlations |
| **Temporal leakage** | Strict chronological splits (no random shuffling) |
| **Data leakage** | Scalers fit on training only, lag features use `shift(1)` |

---

## ğŸ—ï¸ Model Architecture (v4.1f)

```
Input: [Batch, 6, 32, 33]  â† Window=7, 32 features, 33 families
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Feature Encoder (32 â†’ 16 dims per family)  â”‚
â”‚  Linear â†’ ReLU â†’ Dropout (acts as reg.)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ Output: [Batch, 6, 528]  (16 Ã— 33)
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Shared LSTM Encoder                         â”‚
â”‚  Input: 528 â†’ Hidden: 256 Ã— 2 layers        â”‚
â”‚  Learns cross-store temporal patterns        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ Output: [Batch, 256]
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Store Adapter (store_nbr specific)         â”‚
â”‚  Linear â†’ LayerNorm â†’ ReLU â†’ Dropout         â”‚
â”‚  54 separate adapters (one per store)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚ Output: [Batch, 256]
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Family Predictor                            â”‚
â”‚  256 â†’ 128 â†’ 33 (all families)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
Output: [Batch, 33]  â† Log-transformed sales predictions
```

### Parameters

| Component | Parameters |
|-----------|------------|
| Shared LSTM | ~1.6M |
| Store Adapters | ~3.5M |
| Family Predictor | ~34K |
| **Total** | **~5.1M** |

---

## ğŸ“… Version History & Timeline

| Version | Date | Changes | Impact |
|---------|------|---------|--------|
| **v4.1f** | 2026-02-03 | Pre-computed 3D tensors in DataLoader (O(1) lookups) | 10-100Ã— faster data loading |
| **v4.1e** | 2026-02-02 | Fixed feature-target scale mismatch with `log1p()` on sales-dependent features | Better alignment between features and target |
| **v4.1c** | 2026-02-02 | Fixed rolling feature edge effects (`min_periods=window`) | Eliminated NaN spikes at series boundaries |
| **v4.1b** | 2026-02-02 | Window size optimization: `window_size=7` (was 14) | **6.08% better**, 4Ã— faster training |
| **v4.0** | 2026-02-01 | Meta-Learning Multi-Task LSTM (production release) | Baseline architecture |

---

## ğŸ› ï¸ Feature Engineering Pipeline

### 1. External Data Merging
- Oil prices (daily)
- Holidays/events (national, regional, local)
- Transactions (per store)
- Store metadata (cluster, city, state, type)

### 2. Temporal Features
```python
day, month, quarter, year, day_of_week, is_weekend
```

### 3. Lag & Rolling Features (Data Leakage Safe!)
```python
# CRITICAL: Always use shift(1) for PAST data only
sale_lag = df.groupby(['store_nbr', 'family'])['sales'].shift(1)

rolling_mean_7 = df.groupby(['store_nbr', 'family'])['sales'].transform(
    lambda x: x.shift(1).rolling(window=7, min_periods=7).mean()
)
```

### 4. Holiday Features
- `is_holiday`, `is_national_holiday`
- 6 one-hot encoded holiday types

### 5. Store Features
- `cluster`, `city_encoded`, `state_encoded`
- 5 one-hot encoded store types

### 6. Scaling & Encoding
- **Numerical**: `StandardScaler` (fit on train only!)
- **Categorical**: `LabelEncoder` for city, state
- **Target**: `log1p(sales)` for RMSLE optimization

### Feature Count: 32 numerical features

---

## ğŸš€ Training Strategy

### Mixed Batch Training

```python
for epoch in range(NUM_EPOCHS):
    random.shuffle(stores_list)  # Different order each epoch
    for store_id in stores_list:
        train_loader, _ = store_loaders[store_id]
        for X_batch, y_batch in train_loader:
            predictions = model(X_batch, store_nbr=store_id)
            loss = criterion(predictions, y_batch)
            loss.backward()
            optimizer.step()
            break  # One batch per store per epoch
```

### Hyperparameters

| Parameter | Value |
|-----------|-------|
| Optimizer | AdamW |
| Learning Rate | 0.00025 |
| Weight Decay | 0.05 |
| Scheduler | ReduceLROnPlateau (factor=0.5, patience=3) |
| Batch Size | 128 |
| Window Size | 7 (6 timesteps in, 1 prediction) |
| Early Stopping | Patience=5, min_delta=0.001 |
| Gradient Clipping | max_norm=1.0 |

### Loss Function: RMSLE

```python
RMSLE = sqrt(MSE(log(pred + 1), log(actual + 1)))
```

Optimized directly on log-transformed sales.

---

## ğŸ“‚ Project Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train_processed.csv          # Preprocessed training data
â”‚   â”œâ”€â”€ val_processed.csv            # Preprocessed validation data
â”‚   â”œâ”€â”€ scaler_X.pkl                 # Feature scaler (for inference)
â”‚   â””â”€â”€ preprocessing_metadata.pkl   # Feature names & encoding info
â”œâ”€â”€ models/
â”‚   â””â”€â”€ best_model_multitask.pt      # Saved model checkpoint
â”œâ”€â”€ ML_Models/
â”‚   â””â”€â”€ multitask_lstm.py            # Model architecture (v4.1f)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_multitask.py            # 3D tensor DataLoader
â”‚   â”œâ”€â”€ training.py                  # Training utilities
â”‚   â””â”€â”€ new_store_utils.py           # Store-specific helpers
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py                    # Logging configuration
â”œâ”€â”€ train_model.py                   # Training script
â”œâ”€â”€ inference.py                     # Kaggle submission generator
â””â”€â”€ create_processed_data.py         # Feature engineering pipeline
```

---

## ğŸƒ Quick Start

### 1. Create Processed Data
```bash
python create_processed_data.py
```
Output: `data/train_processed.csv`, `data/val_processed.csv`

### 2. Train Model
```bash
python train_model.py
```
Output: `models/best_model_multitask.pt`

### 3. Generate Submission
```bash
python inference.py
```
Output: `submission.csv`

---

## âš™ï¸ Configuration

### Device: Apple Silicon (MPS)
```python
device = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
```

### Training/Validation Split
```python
split_date = '2017-07-15'  # Chronological split (no shuffling!)
train = df[df.index < split_date]
val = df[df.index >= split_date]
```

---

## ğŸ“ Key Learnings

### What Worked

1. **Store adapters** - Critical for handling store heterogeneity
2. **Window size 7** - Optimal balance (smaller was worse, larger was slower)
3. **Feature compression** - 32â†’16 dims prevented overfitting
4. **Pre-computed tensors** - Massive data loading speedup
5. **Log-transform on sales-dependent features** - Fixed feature-target scale mismatch

### What Didn't Work

1. **Larger windows** (14, 30) - More noise, slower, worse performance
2. **No store adapters** - Stores too different for shared model only
3. **Random shuffling** - Temporal leakage destroyed generalization
4. **Fitting scalers on full data** - Data leakage inflated validation scores

---

## ğŸ“Š Performance

### Training Speed (v4.1f)
- **Data loading**: 10-100Ã— faster (pre-computed tensors)
- **Epoch time**: ~20-30 seconds (54 stores, mixed batch)

### Model Metrics
- **Parameters**: ~5.1M trainable
- **Input**: 6 timesteps Ã— 32 features Ã— 33 families
- **Output**: 33 family sales predictions

---

## ğŸ”§ Development Commands

```bash
# Linting (critical errors only)
flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics

# Format code
black . --line-length 100

# Train from scratch
python train_model.py

# Resume from checkpoint (auto-detects)
python train_model.py
```

---

## ğŸ“ License

This project is part of a Kaggle competition. See [Kaggle Terms](https://www.kaggle.com/terms) for competition-specific rules.

---

## ğŸ¤ Contributing

This is a solo competition project. For questions or issues, please open an issue on GitHub.
